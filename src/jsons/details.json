{
    "BATI": {
        "name": "Balanced Accuracy Time Index", 
        "type": "classification",
        "formula": "BATI=weightAcc~*~normalizedAcc~+~weightTime~*~(1~-~normalizedTime)", 
        "formulaBreakdown": {
            "1": "weightAcc = User assigned performance weight", 
            "2": "weightTime = User assigned external factor weight", 
            "3": "normalizedAcc = Min-max normalization of multiple model's accuracy", 
            "4": "normalizedTime = Min-max normalization of multiple model's time duration (to train or test)"
        },
        "description": "Balanced Accuracy Time Index is a metric that considers both the accuracy of the model and the time it takes to either train or test (whichever the user values more based on their circumstances). By default, the weights for this, and all other metrics are set at a default value of 0.5 to give equal balance to both the performance factor and external factor, these weights can be adjusted as needed to better reflect the user's prioritizations. The values of accuracy and time are normalized using min-max normalization, the purpose for normalization is to allow the metric to account for multiple separate models each of which having their own respective accuracy and time duration. Output values near 1 are considered very good and values near 0 are considered very bad."
    },
    "RAF1_CPU" : {
        "name": "Resource-Adjusted F1 Score (CPU)", 
        "type": "classification",
        "formula": "RAF1(CPU)=(weight F1~*~normalizedF1Score) + (weightCpu~*~(1~-~\\frac{averageCpuUsage~-~minCpuUsage}{maxCpuUsage~-~minCpuUsage}))", 
        "formulaBreakdown": {
            "1": "weightF1 = User assigned performance weight", 
            "2": "weightCpu = User assigned external factor weight", 
            "3": "normalizedF1Score = Min-max normalization of multiple model's F1 scores", 
            "4": "averageCpuUsage = Average CPU usage of the chosen model during observation period",
            "5": "minCpuUsage = Minimum CPU usage of any model during observation period",
            "6": "maxCpuUsage = Maximum CPU usage of any model during observation period"
        },
        "description": "Resource-Adjusted F1 Score (CPU) is a metric that provides a comparison between a system resource (in this case CPU% usage) and the performance of the model in question with its F1 Score. Due to the varying nature of system resource usage when doing any task, multiple measurements have to be taken across multiple epochs to get an overall average resource usage during that time frame. Both the performance and external factor are normalized across all the models being compared using min-max normalization in order to give insights to which model performs the best with the current conditions and with the chosen weights set by the user. The addition of '1-' to the resource normalization is to make the formula treat lower use of resources as a positive and higher resource usage to be considered as a negative. Default weights are 0.5 for both performance and external factor."
    },
    "RAF1_GPU" : {
        "name": "Resource-Adjusted F1 Score (GPU)", 
        "type": "classification",
        "formula": "RAF1(GPU)=(weight F1~*~normalizedF1Score) + (weightGpu~*~(1~-~\\frac{averageGpuUsage~-~minGpuUsage}{maxGpuUsage~-~minGpuUsage}))", 
        "formulaBreakdown": {
            "1": "weightF1 = User assigned performance weight", 
            "2": "weightGpu = User assigned external factor weight", 
            "3": "normalizedF1Score = Min-max normalization of multiple model's F1 scores", 
            "4": "averageGpuUsage = Average GPU usage of the chosen model during observation period",
            "5": "minGpuUsage = Minimum GPU usage of any model during observation period",
            "6": "maxGpuUsage = Maximum GPU usage of any model during observation period"
        },
        "description": "Resource-Adjusted F1 Score (GPU) is a metric that provides a comparison between a system resource (in this case GPU VRAM usage) and the performance of the model in question with its F1 Score. Due to the varying nature of system resource usage when doing any task, multiple measurements have to be taken across multiple epochs to get an overall average resource usage during that time frame. Both the performance and external factor are normalized across all the models being compared using min-max normalization in order to give insights to which model performs the best with the current conditions and with the chosen weights set by the user. The addition of '1-' to the resource normalization is to make the formula treat lower use of resources as a positive and higher resource usage to be considered as a negative. Default weights are 0.5 for both performance and external factor."
    },
    "RAF1_RAM" : {
        "name": "Resource-Adjusted F1 Score (RAM)", 
        "type": "classification",
        "formula": "RAF1(RAM)=(weight F1~*~normalizedF1Score) + (weightRam~*~(1~-~\\frac{averageRamUsage~-~minRamUsage}{maxRamUsage~-~minRamUsage}))", 
        "formulaBreakdown": {
            "1": "weightF1 = User assigned performance weight", 
            "2": "weightRam = User assigned external factor weight", 
            "3": "normalizedF1Score = Min-max normalization of multiple model's F1 scores", 
            "4": "averageRamUsage = Average RAM usage of the chosen model during observation period",
            "5": "minRamUsage = Minimum RAM usage of any model during observation period",
            "6": "maxRamuUsage = Maximum RAM usage of any model during observation period"
        },
        "description": "Resource-Adjusted F1 Score (RAM) is a metric that provides a comparison between a system resource (in this case RAM usage) and the performance of the model in question with its F1 Score. Due to the varying nature of system resource usage when doing any task, multiple measurements have to be taken across multiple epochs to get an overall average resource usage during that time frame. Both the performance and external factor are normalized across all the models being compared using min-max normalization in order to give insights to which model performs the best with the current conditions and with the chosen weights set by the user. The addition of '1-' to the resource normalization is to make the formula treat lower use of resources as a positive and higher resource usage to be considered as a negative. Default weights are 0.5 for both performance and external factor."
    },
    "F1CI" : {
        "name": "F1-Cost Index", 
        "type": "classification",
        "formula": "F1CI=\\frac{2~*~costTP~*~TP}{2~*~costTP~*~TP~+~costFP~*~FP~+~costFN~*~FN}", 
        "formulaBreakdown": {
            "1": "TP, FP, FN = True Positive, False Positive, False Negative", 
            "2": "costTP, costFP, costFN = Estimated costs to an entity for True Positive, False Positive, and False Negative predictions"
        },
        "description": "The metric F1-Cost Index is based on the Safety Score metric developed by (Salman et al., 2020) in which they added cost weights to the True Positive, True Negative, False Positive, and False Negative parameters of the standard accuracy equation to gauge how much it would cost the individual / corporation should the model in question predicate a specific output. The cost values would be set by a system administrator in collaboration with crisis management employees. For example, let us say that TP costs one thousand dollars, TN cost one thousand dollars, FP cost ten thousand dollars, and FN cost twenty thousand dollars. In this scenario when the Safety score was calculated the few instances of FP and FN would heavily weigh on the metrics output value, making it much more sensitive based on the few mistakes made by the model. In this vein, the author has noted that while this metric is useful and provides valuable information, the base metric of choice has much to be desired, as accuracy can be very unreliable at times, especially since it is susceptible to bias in the case of an imbalanced dataset. Therefore, the author has chosen to utilize F1 score instead of accuracy in order to give more reliable results that are less biased in imbalanced datasets."
    },
    "BRMSETI" : {
        "name": "Balanced RMSE Time Index", 
        "type": "regression",
        "formula": "BRMSETI=weightRmse~*~(1-normalizedRmse)~+~weightTime~*~(1~-~normalizedTime)", 
        "formulaBreakdown": {
            "1": "weightRmse = User assigned performance weight", 
            "2": "weightTime = User assigned external factor weight", 
            "3": "normalizedRmse = Min-max normalization of multiple model's RMSE", 
            "4": "normalizedTime = Min-max normalization of multiple model's time duration (to train or test)"
        },
        "description": "Balanced RMSE Time Index is a metric that considers both the RMSE of the model and the time it takes to either train or test (whichever the user values more based on their circumstances). By default, the weights for this, and all other metrics are set at a default value of 0.5 to give equal balance to both the performance factor and external factor, these weights can be adjusted as needed to better reflect the user's prioritizations. The values of RMSE and time are normalized using min-max normalization, the purpose for normalization is to allow the metric to account for multiple separate models each of which having their own respective RMSE and time duration. Output values near 1 are considered very good and values near 0 are considered very bad."
    },
    "RARS_CPU" : {
        "name": "Resource-Adjusted R-Squared (CPU)", 
        "type": "regression",
        "formula": "RARS(CPU)=(weightRS~*~normalizedRSquared) + (weightCpu~*~(1~-~\\frac{averageCpuUsage~-~minCpuUsage}{maxCpuUsage~-~minCpuUsage}))", 
        "formulaBreakdown": {
            "1": "weightRS = User assigned performance weight", 
            "2": "weightCpu = User assigned external factor weight", 
            "3": "normalizedRSScore = Min-max normalization of multiple model's R-Squared scores", 
            "4": "averageCpuUsage = Average CPU usage of the chosen model during observation period",
            "5": "minCpuUsage = Minimum CPU usage of any model during observation period",
            "6": "maxCpuUsage = Maximum CPU usage of any model during observation period"
        },
        "description": "Resource-Adjusted R-Squared (CPU) is a metric that provides a comparison between a system resource (in this case CPU% usage) and the performance of the model in question with its R-Squared score. Due to the varying nature of system resource usage when doing any task, multiple measurements have to be taken across multiple epochs to get an overall average resource usage during that time frame. Both the performance and external factor are normalized across all the models being compared using min-max normalization in order to give insights to which model performs the best with the current conditions and with the chosen weights set by the user. The addition of '1-' to the resource normalization is to make the formula treat lower use of resources as a positive and higher resource usage to be considered as a negative. Default weights are 0.5 for both performance and external factor."
    },
    "RARS_GPU" : {
        "name": "Resource-Adjusted R-Squared (GPU)", 
        "type": "regression",
        "formula": "RARS(GPU)=(weight RS~*~normalizedRSquared) + (weightGpu~*~(1~-~\\frac{averageGpuUsage~-~minGpuUsage}{maxGpuUsage~-~minGpuUsage}))", 
        "formulaBreakdown": {
            "1": "weightRS = User assigned performance weight", 
            "2": "weightGpu = User assigned external factor weight", 
            "3": "normalizedRSScore = Min-max normalization of multiple model's R-Squared scores",  
            "4": "averageGpuUsage = Average GPU usage of the chosen model during observation period",
            "5": "minGpuUsage = Minimum GPU usage of any model during observation period",
            "6": "maxGpuUsage = Maximum GPU usage of any model during observation period"
        },
        "description": "Resource-Adjusted R-Squared (GPU) is a metric that provides a comparison between a system resource (in this case GPU VRAM usage) and the performance of the model in question with its R-Squared score. Due to the varying nature of system resource usage when doing any task, multiple measurements have to be taken across multiple epochs to get an overall average resource usage during that time frame. Both the performance and external factor are normalized across all the models being compared using min-max normalization in order to give insights to which model performs the best with the current conditions and with the chosen weights set by the user. The addition of '1-' to the resource normalization is to make the formula treat lower use of resources as a positive and higher resource usage to be considered as a negative. Default weights are 0.5 for both performance and external factor."
    },
    "RARS_RAM" : {
        "name": "Resource-Adjusted R-Squared (RAM)", 
        "type": "regression",
        "formula": "RARS(RAM)=(weight RS~*~normalizedRSquared) + (weightRam~*~(1~-~\\frac{averageRamUsage~-~minRamUsage}{maxRamUsage~-~minRamUsage}))", 
        "formulaBreakdown": {
            "1": "weightRS = User assigned performance weight", 
            "2": "weightRam = User assigned external factor weight", 
            "3": "normalizedRSScore = Min-max normalization of multiple model's R-Squared scores",
            "4": "averageRamUsage = Average RAM usage of the chosen model during observation period",
            "5": "minRamUsage = Minimum RAM usage of any model during observation period",
            "6": "maxRamuUsage = Maximum RAM usage of any model during observation period"
        },
        "description": "Resource-Adjusted R-Squared (RAM) is a metric that provides a comparison between a system resource (in this case RAM usage) and the performance of the model in question with its R-Squared score. Due to the varying nature of system resource usage when doing any task, multiple measurements have to be taken across multiple epochs to get an overall average resource usage during that time frame. Both the performance and external factor are normalized across all the models being compared using min-max normalization in order to give insights to which model performs the best with the current conditions and with the chosen weights set by the user. The addition of '1-' to the resource normalization is to make the formula treat lower use of resources as a positive and higher resource usage to be considered as a negative. Default weights are 0.5 for both performance and external factor."
    },
    "RMSPECI" : {
        "name": "RMSPE-Cost Index", 
        "type": "regression",
        "formula": "RMSPECI=costPerPercent~*~RMSPE", 
        "formulaBreakdown": {
            "1": "RMSPE = Root Mean Squared Percentage Error", 
            "2": "costPerPercent = Estimated cost per percentage error differnce"
        },
        "description": "In the vein of the F1CI metric, the RMSPE-Cost Index is a regression interpretation of the metric where instead of assigning weights to each of the parameters of the confusion matrix, instead the system admin has to approximate the cost per percentage difference from the actual result. This is multiplied by the RMSPE which is the percentage difference between the predicted results and actual result of the regression model."
    },
    "Model_Robustness": {
        "name": "Model Robustness", 
        "type": "classification",
        "formula": "../images/robustGraph.png", 
        "formulaBreakdown": "",
        "description": "Model Robustness Index is a metric portrayed in graphical form and its focus is on the comparison of the modelâ€™s performance to percentage noise in the dataset, this was done in order to show how a model will hold up performance wise in scenarios where the dataset is gradually corrupted with noise or garbage data. The graphical metric has either F1 Score or R-Squared on the Y axis depending if the model is classification or regression based, and Noise% on the X axis."
    }
}